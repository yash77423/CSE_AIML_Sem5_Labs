{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/14 15:43:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string, _c32: string, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string, _c43: string, _c44: string, _c45: string, _c46: string, _c47: string, _c48: string, _c49: string, _c50: string, _c51: string, _c52: string, _c53: string, _c54: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv('covtype.data')\n",
    "data.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "            \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_noon\", \"Hillshade_3pm\",\n",
    "            \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "           [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n",
    "\n",
    "data = data.toDF(*colnames)\n",
    "data = data.withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
    "for name in colnames[:-1]:\n",
    "    data = data.withColumn(name, col(name).cast(IntegerType()))\n",
    "data = data.na.drop()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "24/09/14 15:43:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_b791574fb87d, depth=5, numNodes=43, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3047.5)\n",
      "   If (feature 0 <= 2500.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 0 <= 2344.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 0 > 2344.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1318.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1318.5)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2500.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 0 <= 2952.5)\n",
      "      If (feature 15 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 15 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2952.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2711.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2711.5)\n",
      "      If (feature 5 <= 1228.0)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1228.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3047.5)\n",
      "   If (feature 0 <= 3311.5)\n",
      "    If (feature 7 <= 240.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 240.5)\n",
      "     If (feature 3 <= 300.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 300.5)\n",
      "      If (feature 0 <= 3203.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3203.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3311.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 280.0)\n",
      "      If (feature 6 <= 207.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 207.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 280.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 910.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 910.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "train_data, test_data = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                                        |\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.003931847968545216,0.30930537352555704,0.5943643512450852,0.0,0.09239842726081259,0.0]                                  |\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.152187618207036E-5,0.0683709494389106,0.6082461228092296,0.020016391375614676,0.0017021813138317992,0.30163283318623124,0.0]|\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                       (0 + 18) / 18]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7026128156944992\n",
      "Precision: 0.7010685345593746\n",
      "Recall: 0.7026128156944991\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
