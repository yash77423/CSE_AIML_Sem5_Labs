{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14d7fc4",
   "metadata": {},
   "source": [
    "# Lab 2: Introduction to Basic PySpark Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47974b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ba1b2",
   "metadata": {},
   "source": [
    "1. Squaring a Set of Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f5ce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|number|squared|\n",
      "+------+-------+\n",
      "|     1|    1.0|\n",
      "|     2|    4.0|\n",
      "|     3|    9.0|\n",
      "|     4|   16.0|\n",
      "|     5|   25.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SquareIntegers\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a set of integers\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "# Square the integers\n",
    "# squared_df = df.withColumn(\"squared\", col(\"number\") ** 2)\n",
    "squared_df = df.select('number',(col('number') ** 2).alias('squared'))\n",
    "\n",
    "# Show the result\n",
    "squared_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066642fc",
   "metadata": {},
   "source": [
    "For simple operations like squaring a number, using built-in functions is usually preferred due to better performance and simplicity. UDFs are more useful for complex operations where built-in functions cannot achieve the desired result.\n",
    "\n",
    "If you need to use a UDF due to custom logic, be mindful of the potential performance overhead and test the performance impact if working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96beb15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|number|squared|\n",
      "+------+-------+\n",
      "|     1|      1|\n",
      "|     2|      4|\n",
      "|     3|      9|\n",
      "|     4|     16|\n",
      "|     5|     25|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"UDFExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "# Define the Python function\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# Convert the Python function into a UDF\n",
    "square_udf = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "# Apply the UDF to create a new column with squared values\n",
    "# squared_df = df.withColumn(\"squared\", square_udf(\"number\"))\n",
    "squared_df = df.select('number',square_udf('number').alias('squared'))\n",
    "\n",
    "# Show the result\n",
    "squared_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ead647",
   "metadata": {},
   "source": [
    "2. Finding the Maximum of a Given Set of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0f4ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum value is: 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FindMaximum\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a set of numbers\n",
    "data = [(1,), (5,), (3,), (4,), (2,)]\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "# Find the maximum number\n",
    "# max_value = df.agg(max(\"number\").alias(\"max_number\")).collect()[0][\"max_number\"]\n",
    "max_value = df.select(\"number\").rdd.max()[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The maximum value is: {max_value}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329c533",
   "metadata": {},
   "source": [
    "3. Finding the Average of N Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7955c3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average value is: 3.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FindAverage\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a set of numbers\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "df = spark.createDataFrame(data, [\"number\"])\n",
    "\n",
    "# Calculate the average\n",
    "average_value = df.agg(avg(\"number\").alias(\"average_number\")).collect()[0][\"average_number\"]\n",
    "\n",
    "# Print the result\n",
    "print(f\"The average value is: {average_value}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1034ff2",
   "metadata": {},
   "source": [
    "4. Reading a CSV File into a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fa3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"test1.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7521d32",
   "metadata": {},
   "source": [
    "5. Displaying the First Few Rows and Schema of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430cdeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|Carol|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DisplayRowsAndSchema\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Carol\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Display the first few rows\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8e392",
   "metadata": {},
   "source": [
    "6. Calculating Basic Summary Statistics for a Specific Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cecc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|summary| id|value|\n",
      "+-------+---+-----+\n",
      "|  count|  3|    3|\n",
      "|   mean|2.0| 20.0|\n",
      "| stddev|1.0| 10.0|\n",
      "|    min|  1| 10.0|\n",
      "|    25%|  1| 10.0|\n",
      "|    50%|  2| 20.0|\n",
      "|    75%|  3| 30.0|\n",
      "|    max|  3| 30.0|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SummaryStatistics\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [(1, 10.0), (2, 20.0), (3, 30.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "# Calculate basic summary statistics for the 'value' column\n",
    "# summary_stats = df.describe(\"value\")\n",
    "summary_stats = df.summary()\n",
    "\n",
    "# Show the summary statistics\n",
    "summary_stats.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf13bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
