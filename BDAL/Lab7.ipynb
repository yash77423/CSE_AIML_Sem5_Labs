{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c4a2af",
   "metadata": {},
   "source": [
    "# 1. Handling Missing Values and Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb38102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     scaled_features|\n",
      "+--------------------+\n",
      "|[-1.4605934866804...|\n",
      "|[-1.0954451150103...|\n",
      "|[-0.7302967433402...|\n",
      "|[-0.3651483716701...|\n",
      "|[0.0,-1.286034203...|\n",
      "|[0.36514837167011...|\n",
      "|[0.73029674334022...|\n",
      "|[1.09544511501033...|\n",
      "|[1.46059348668044...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"DataPreprocessing\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "data = spark.read.csv(\"data1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Handle missing values using Imputer\n",
    "imputer = Imputer(inputCols=data.columns, outputCols=[f\"{col}_imputed\" for col in data.columns])\n",
    "data_imputed = imputer.fit(data).transform(data)\n",
    "\n",
    "# Assemble features into a vector\n",
    "feature_cols = [f\"{col}_imputed\" for col in data.columns]  # Use imputed columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data_vector = assembler.transform(data_imputed)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(data_vector)\n",
    "data_scaled = scaler_model.transform(data_vector)\n",
    "\n",
    "# Show the result\n",
    "data_scaled.select(\"scaled_features\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73dad4",
   "metadata": {},
   "source": [
    "# 2. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e9b120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|     scaled_features|prediction|\n",
      "+--------------------+----------+\n",
      "|[-1.4605934866804...|         1|\n",
      "|[-1.0954451150103...|         1|\n",
      "|[-0.7302967433402...|         2|\n",
      "|[-0.3651483716701...|         2|\n",
      "|[0.0,-1.286034203...|         1|\n",
      "|[0.36514837167011...|         2|\n",
      "|[0.73029674334022...|         0|\n",
      "|[1.09544511501033...|         0|\n",
      "|[1.46059348668044...|         0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Set the number of clusters\n",
    "num_clusters = 3\n",
    "\n",
    "# Train K-means model\n",
    "kmeans = KMeans(featuresCol=\"scaled_features\", k=num_clusters)\n",
    "model = kmeans.fit(data_scaled)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(data_scaled)\n",
    "\n",
    "# Show cluster assignments\n",
    "predictions.select(\"scaled_features\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d31bdf",
   "metadata": {},
   "source": [
    "# 3. Labeling Data Points as Anomalies Based on Cluster Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4ed02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|     scaled_features|prediction|is_anomaly|\n",
      "+--------------------+----------+----------+\n",
      "|[-1.4605934866804...|         1|         0|\n",
      "|[-1.0954451150103...|         1|         0|\n",
      "|[-0.7302967433402...|         2|         1|\n",
      "|[-0.3651483716701...|         2|         1|\n",
      "|[0.0,-1.286034203...|         1|         0|\n",
      "|[0.36514837167011...|         2|         1|\n",
      "|[0.73029674334022...|         0|         1|\n",
      "|[1.09544511501033...|         0|         1|\n",
      "|[1.46059348668044...|         0|         1|\n",
      "+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming a point is an anomaly if it is not assigned to the most populous cluster\n",
    "# Get the count of points in each cluster\n",
    "cluster_counts = predictions.groupBy(\"prediction\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Get the most populous cluster\n",
    "most_populous_cluster = cluster_counts.first()[\"prediction\"]\n",
    "\n",
    "# Label anomalies (1 for anomaly, 0 for normal)\n",
    "predictions = predictions.withColumn(\"is_anomaly\", F.when(predictions.prediction != most_populous_cluster, 1).otherwise(0))\n",
    "\n",
    "# Show results\n",
    "predictions.select(\"scaled_features\", \"prediction\", \"is_anomaly\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f2172",
   "metadata": {},
   "source": [
    "# 4. Evaluate the Effectiveness of K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e30ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.8060494821051775\n",
      "Number of anomalies detected: 6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"scaled_features\", predictionCol=\"prediction\")\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette}\")\n",
    "\n",
    "# Show the number of anomalies detected\n",
    "num_anomalies = predictions.filter(predictions.is_anomaly == 1).count()\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7433bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
