{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0ba3ad",
   "metadata": {},
   "source": [
    "# Lab 3: Simple PySpark Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b234d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2a6c1",
   "metadata": {},
   "source": [
    "1. Applying Transformations (Filter and withColumn) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421b9b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------------+\n",
      "| id| name|age|age_plus_ten|\n",
      "+---+-----+---+------------+\n",
      "|  1|Alice| 29|          39|\n",
      "|  2|  Bob| 35|          45|\n",
      "|  3|Carol| 30|          40|\n",
      "+---+-----+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TransformationsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 35), (3, \"Carol\", 30), (4, \"David\", 25)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Apply transformations\n",
    "# Filter rows where age is greater than 28\n",
    "filtered_df = df.filter(col(\"age\") > 28)\n",
    "\n",
    "# Add a new column 'age_plus_ten' which is 'age' + 10\n",
    "transformed_df = filtered_df.withColumn(\"age_plus_ten\", col(\"age\") + 10)\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d69dc4",
   "metadata": {},
   "source": [
    "2. Performing Actions (count and show) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bd2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|Carol|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ActionsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Carol\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Perform actions\n",
    "# Count the number of rows\n",
    "count = df.count()\n",
    "print(f\"Number of rows: {count}\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2ab6d",
   "metadata": {},
   "source": [
    "3. Performing Basic Aggregations (e.g., Sum, Average) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7c849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|total_value|average_value|\n",
      "+-----------+-------------+\n",
      "|       60.0|         20.0|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AggregationsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 10.0), (2, 20.0), (3, 30.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "# Perform basic aggregations\n",
    "# Calculate sum and average of the 'value' column\n",
    "aggregations = df.agg(\n",
    "    sum(\"value\").alias(\"total_value\"),\n",
    "    avg(\"value\").alias(\"average_value\")\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "aggregations.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4285247",
   "metadata": {},
   "source": [
    "4. Writing a PySpark DataFrame to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab39b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WriteSingleCSV\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 35), (3, \"Carol\", 30)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Path to write the CSV file\n",
    "csv_file_path = \"l3q4.csv\"\n",
    "\n",
    "# Coalesce the DataFrame to a single partition\n",
    "single_partition_df = df.coalesce(1)\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "single_partition_df.write.csv(csv_file_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4406f",
   "metadata": {},
   "source": [
    "5. Word Count Program in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e796910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|  Hello|    2|\n",
      "|  world|    1|\n",
      "|PySpark|    1|\n",
      "|example|    1|\n",
      "|  count|    1|\n",
      "|   Word|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Hello world\",), (\"Hello PySpark\",), (\"Word count example\",)]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "\n",
    "# Perform word count\n",
    "# Split the text into words and explode into separate rows\n",
    "words_df = df.withColumn(\"word\", explode(split(col(\"text\"), \" \")))\n",
    "\n",
    "# Group by word and count occurrences\n",
    "word_counts = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Show the word counts\n",
    "word_counts.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ec55d",
   "metadata": {},
   "source": [
    "Alternate Method(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44d05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c31da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 16:14:39 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:76)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana:5000\n",
      "cherry:5000\n",
      "elderberry:5000\n",
      "honeydew:5000\n",
      "apple:5000\n",
      "date:5000\n",
      "fig:5000\n",
      "grape:5000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load the text file\n",
    "lines = sc.textFile(\"sample.txt\")\n",
    "\n",
    "# Perform the word count\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "              .map(lambda word: (word, 1)) \\\n",
    "              .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and print the results\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(f\"{word}:{count}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b456e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 16:19:01 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:76)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:53)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:484)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|     grape| 5000|\n",
      "|elderberry| 5000|\n",
      "|     apple| 5000|\n",
      "|    cherry| 5000|\n",
      "|    banana| 5000|\n",
      "|      date| 5000|\n",
      "|  honeydew| 5000|\n",
      "|       fig| 5000|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "# Create a SparkSession with configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the text file\n",
    "lines = spark.read.text(\"sample.txt\")\n",
    "\n",
    "# Split the lines into words and perform word count\n",
    "words = lines.withColumn('word', f.explode(f.split(f.col('value'), ' '))) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .sort(f.col('count').desc()) \\\n",
    "    .show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
