{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0ba3ad",
   "metadata": {},
   "source": [
    "# Lab 3: Simple PySpark Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b234d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2a6c1",
   "metadata": {},
   "source": [
    "1. Applying Transformations (Filter and withColumn) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421b9b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------------+\n",
      "| id| name|age|age_plus_ten|\n",
      "+---+-----+---+------------+\n",
      "|  1|Alice| 29|          39|\n",
      "|  2|  Bob| 35|          45|\n",
      "|  3|Carol| 30|          40|\n",
      "+---+-----+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TransformationsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 35), (3, \"Carol\", 30), (4, \"David\", 25)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Apply transformations\n",
    "# Filter rows where age is greater than 28\n",
    "filtered_df = df.filter(col(\"age\") > 28)\n",
    "\n",
    "# Add a new column 'age_plus_ten' which is 'age' + 10\n",
    "transformed_df = filtered_df.withColumn(\"age_plus_ten\", col(\"age\") + 10)\n",
    "\n",
    "# Show the result\n",
    "transformed_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d69dc4",
   "metadata": {},
   "source": [
    "2. Performing Actions (count and show) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bd2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|Carol|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ActionsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Carol\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "# Perform actions\n",
    "# Count the number of rows\n",
    "count = df.count()\n",
    "print(f\"Number of rows: {count}\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2ab6d",
   "metadata": {},
   "source": [
    "3. Performing Basic Aggregations (e.g., Sum, Average) on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7c849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|total_value|average_value|\n",
      "+-----------+-------------+\n",
      "|       60.0|         20.0|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AggregationsExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 10.0), (2, 20.0), (3, 30.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "\n",
    "# Perform basic aggregations\n",
    "# Calculate sum and average of the 'value' column\n",
    "aggregations = df.agg(\n",
    "    sum(\"value\").alias(\"total_value\"),\n",
    "    avg(\"value\").alias(\"average_value\")\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "aggregations.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4285247",
   "metadata": {},
   "source": [
    "4. Writing a PySpark DataFrame to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab39b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WriteSingleCSV\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, \"Alice\", 29), (2, \"Bob\", 35), (3, \"Carol\", 30)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Path to write the CSV file\n",
    "csv_file_path = \"l3q4.csv\"\n",
    "\n",
    "# Coalesce the DataFrame to a single partition\n",
    "single_partition_df = df.coalesce(1)\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "single_partition_df.write.csv(csv_file_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4406f",
   "metadata": {},
   "source": [
    "5. Word Count Program in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e796910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|  Hello|    2|\n",
      "|  world|    1|\n",
      "|PySpark|    1|\n",
      "|example|    1|\n",
      "|  count|    1|\n",
      "|   Word|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Hello world\",), (\"Hello PySpark\",), (\"Word count example\",)]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "\n",
    "# Perform word count\n",
    "# Split the text into words and explode into separate rows\n",
    "words_df = df.withColumn(\"word\", explode(split(col(\"text\"), \" \")))\n",
    "\n",
    "# Group by word and count occurrences\n",
    "word_counts = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Show the word counts\n",
    "word_counts.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d05b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
